[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "digiqual",
    "section": "",
    "text": "digiqual is a Python library designed for Non-Destructive Evaluation (NDE) engineers. It provides a robust statistical framework for performing Model-Assisted Probability of Detection (MAPOD) studies and reliability assessments. The package is built to implement the Generalised \\(\\hat{a}\\)-versus-\\(a\\) Method, allowing users to assess inspection reliability even when traditional assumptions (linearity, constant variance, Gaussian noise) are not met.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#core-features",
    "href": "index.html#core-features",
    "title": "digiqual",
    "section": "Core Features",
    "text": "Core Features\n\n1. Experimental Design\nBefore running expensive Finite Element (FE) simulations, digiqual helps you design your experiment efficiently.\n\nLatin Hypercube Sampling (LHS): Generate space-filling experimental designs to cover your deterministic parameter space (e.g., defect size) and stochastic nuisance parameters (e.g., roughness, orientation).\nScale & Bound: Automatically scale samples to your specific variable bounds.\n\n\n\n2. Data Validation & Diagnostics\nEnsure your simulation outputs are statistically valid before processing.\n\nSanity Checks: Detects overlap between variables, type errors, and insufficient sample sizes.\nSufficiency Diagnostics: rigorous statistical tests to flag issues like “Input Coverage Gaps” or “Model Instability” before you trust the results.\n\n\n\n3. Adaptive Refinement & Automated Optimisation\ndigiqual closes the loop between analysis and design.\n\nSmart Refinement: Use refine() to identify specific weaknesses in your data. It uses bootstrap committees to find regions of high uncertainty and suggests new points exactly where the model is “confused”.\nAutomated Workflows: Use the optimise() method to run a fully automated “Active Learning” loop. It generates an initial design, executes your external solver, checks diagnostics, and iteratively refines the model until statistical requirements are met.\n\n\n\n4. Generalised Reliability Analysis\nThe package includes a full statistical engine for calculating Probability of Detection (PoD) curves.\n\nRelaxed Assumptions: Moves beyond the rigid constraints of the classical \\(\\hat{a}\\)-versus-\\(a\\) method by handling non-linear signal responses and heteroscedastic noise.\nRobust Statistics: Automatically selects the best polynomial degree and error distribution (e.g., Normal, Gumbel, Logistic) based on data fit (AIC).\nUncertainty Quantification: Uses bootstrap resampling to generate robust confidence bounds and \\(a_{90/95}\\) estimates.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "digiqual",
    "section": "References",
    "text": "References\nThis package implements methods described in:\nMalkiel, N., Croxford, A. J., & Wilcox, P. D. (2025). A generalized method for the reliability assessment of safety–critical inspection. Proceedings of the Royal Society A, 481: 20240654. https://doi.org/10.1098/rspa.2024.0654",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/optimisation.html",
    "href": "docs/optimisation.html",
    "title": "Automated Optimisation",
    "section": "",
    "text": "The optimise method allows you to run a complete “Active Learning” loop. DigiQual will generate an initial design, run your external solver, check the results, and automatically add new points where the model is weak.\nInstead of writing for loops yourself, you define the problem and let digiqual drive.",
    "crumbs": [
      "Automation & Engines"
    ]
  },
  {
    "objectID": "docs/optimisation.html#how-it-works-under-the-hood",
    "href": "docs/optimisation.html#how-it-works-under-the-hood",
    "title": "Automated Optimisation",
    "section": "How It Works Under the Hood",
    "text": "How It Works Under the Hood\nWhen you trigger the optimise method, DigiQual acts as an orchestrator for your simulation workflows. It performs the following steps:\n\nInitialisation: If you have no existing data, DigiQual generates an initial batch of input coordinates using Latin Hypercube Sampling (LHS) to ensure a good spread across your variable ranges.\nExecution: It writes these coordinates to a temporary CSV file and commands your external solver to run.\nDiagnostics (Sense): Once the results are back, DigiQual runs statistical checks. It looks for “gaps” in your input coverage and measures “model uncertainty” using a technique called Bootstrap Query-by-Committee.\nRefinement (Decide & Act): If the diagnostics fail, DigiQual generates targeted new samples exactly where the model needs them most (e.g., in the middle of an empty gap, or in highly uncertain regions) and repeats the loop.",
    "crumbs": [
      "Automation & Engines"
    ]
  },
  {
    "objectID": "docs/optimisation.html#connecting-your-external-solver",
    "href": "docs/optimisation.html#connecting-your-external-solver",
    "title": "Automated Optimisation",
    "section": "Connecting Your External Solver",
    "text": "Connecting Your External Solver\nTo automate this process, DigiQual needs to communicate with your external software (like MATLAB, Ansys, or a custom Python script). It does this using a simple file-based contract via a command string.\nYour command string must contain two special placeholders: {input} and {output}.\n\n{input}: DigiQual will replace this with the path to a CSV file it creates. This CSV contains the input variables for the current batch of simulations.\n{output}: DigiQual will replace this with the path where it expects your solver to save the final results as a CSV.\n\n\nThe “Wrapper” Concept\nMost heavy physics solvers don’t natively read and write CSVs in exactly the way DigiQual expects. In real life, you will usually write a small “wrapper script” (e.g., run_my_model.py or run_my_model.bat). See Appendix for full examples python and MATLAB.\nYour wrapper script’s job is to:\n\nRead the DigiQual {input} CSV.\nModify your solver’s native input deck (e.g., updating parameters in an Ansys .mac file).\nTrigger the actual solver in headless/batch mode.\nExtract the results (e.g., the signal or stress value) from the solver’s output files.\nSave a clean CSV to the {output} path with the original inputs and the new outcome column appended.",
    "crumbs": [
      "Automation & Engines"
    ]
  },
  {
    "objectID": "docs/optimisation.html#the-auto-pilot-workflow",
    "href": "docs/optimisation.html#the-auto-pilot-workflow",
    "title": "Automated Optimisation",
    "section": "The Auto-Pilot Workflow",
    "text": "The Auto-Pilot Workflow\nLet’s walk through setting up a complete optimisation loop.\n\n1. Define a “Mock” Solver\nFor this tutorial, we will use a Python one-liner as our “solver” instead of a complex physics engine. We use python -c to run this as if it were an external Command Line Interface (CLI) tool.\nNotice how the SOLVER_CMD utilises the {input} and {output} placeholders.\n\nimport sys\n\n# A complex, non-linear physics model to trigger multiple optimisation loops\nSOLVER_CMD = (\n    f\"{sys.executable} -c \"\n    \"'import pandas as pd, numpy as np; \"\n    'df=pd.read_csv(\"{input}\"); '\n    'df[\"Signal\"] = 5 + df[\"Length\"] ** 3 + (df[\"Length\"] * np.random.normal(0, 1, len(df))); '\n    'df.to_csv(\"{output}\", index=False)\\''\n)\n\n\n\n2. Configure the Study\nWe define our input variables and the ranges we want to explore.\n\nfrom digiqual.core import SimulationStudy\n\n# Define inputs ranges\nranges = {\"Length\": (0.0, 5.0), \"Angle\": (-45.0, 45.0)}\n\n# Initialise\nstudy = SimulationStudy(input_cols=[\"Length\", \"Angle\"], outcome_col=\"Signal\")\n\n\n\n3. Run Optimisation\nThis single command handles the entire Active Learning loop:\n\nGenerates 20 initial LHS points (because study.data is currently empty).\nRuns the solver command, passing the temporary input and output file paths.\nChecks diagnostics to see if the data is sufficient.\nIf needed, generates up to 10 new points targeting weak areas and repeats (up to 5 times).\n\n\nstudy.optimise(\n    command=SOLVER_CMD,\n    ranges=ranges,\n    n_start=20,  # Initial batch size\n    n_step=10,  # Refinement batch size\n    max_iter=5,  # Safety limit for the loop\n)\n\n\n=== STARTING ADAPTIVE OPTIMIZATION ===\n--- Iteration 0: Generating Initial Design (20 points) ---\n   -&gt; Executing: /Users/jt17630/Documents/DigiQual/.venv/bin/python3 -c 'import pandas as pd, numpy as np; df=pd.read_csv(\"sim_input.csv\"); df[\"Signal\"] = 5 + df[\"Length\"] ** 3 + (df[\"Length\"] * np.random.normal(0, 1, len(df))); df.to_csv(\"sim_output.csv\", index=False)'\n\n--- Iteration 1: Diagnostics Check ---\n&gt;&gt; Model invalid. Refining design...\nDiagnostics flagged issues. Initiating Active Learning...\n -&gt; Strategy: Exploitation (Targeting high uncertainty regions)\n--- Running Batch 1 (10 points) ---\n   -&gt; Executing: /Users/jt17630/Documents/DigiQual/.venv/bin/python3 -c 'import pandas as pd, numpy as np; df=pd.read_csv(\"sim_input.csv\"); df[\"Signal\"] = 5 + df[\"Length\"] ** 3 + (df[\"Length\"] * np.random.normal(0, 1, len(df))); df.to_csv(\"sim_output.csv\", index=False)'\n\n--- Iteration 2: Diagnostics Check ---\n\n&gt;&gt;&gt; CONVERGENCE REACHED! &lt;&lt;&lt;\nFinal Report: 30 successful runs (out of 30 attempts). Graveyard contains 0 points.\nData updated. Total rows: 30\n\n\n\n\n4. View Results\nOnce the loop finishes, study.data contains all the valid simulation results accumulated across all refinement iterations.\nprint(f\"Total Simulations Run: {len(study.data)}\")\n_ = study.pod(poi_col=\"Length\", threshold=20)\nstudy.visualise()\n\n\n\n\n\nTotal Simulations Run: 30\nRunning validation...\nValidation passed. 30 valid rows ready.\n--- Starting Reliability Analysis (PoI: Length) ---\n1. Selecting Mean Model (Cross-Validation)...\n-&gt; Selected Model: Polynomial (Degree 3)\n2. Fitting Variance Model (Kernel Smoothing)...\n   -&gt; Smoothing Bandwidth: 0.4722\n3. Inferring Error Distribution (AIC)...\n   -&gt; Selected Distribution: norm\n4. Computing PoD Curve...\n5. Running Bootstrap (1000 iterations)...\n   -&gt; a90/95 Reliability Index: 2.680\n--- Analysis Complete ---\n\n\n\n\n\n\n\n\n\n\n(a) Best Fitting Model (Statistics)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Signal Response Model (Physics)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Probability of Detection Curve (Reliability)\n\n\n\n\n\n\n\nFigure 1: Reliability Analysis Results",
    "crumbs": [
      "Automation & Engines"
    ]
  },
  {
    "objectID": "docs/optimisation.html#sec-appendix",
    "href": "docs/optimisation.html#sec-appendix",
    "title": "Automated Optimisation",
    "section": "Appendix: Real-World Wrapper Examples",
    "text": "Appendix: Real-World Wrapper Examples\nWhen connecting DigiQual to external physics engines or legacy code, you rarely call the solver directly. Instead, you create a “wrapper script.”\nThe wrapper’s job is simple:\n\nAccept the {input} and {output} file paths from DigiQual.\nRead the {input} CSV.\nTranslate those inputs into the format your solver expects (e.g., updating variables, modifying text files).\nRun the solver.\nExtract the results and save everything to the {output} CSV.\n\nBelow are examples of how to achieve this in Python and MATLAB.\n\nExample A: Connecting a Custom Python Model\nImagine you have an existing Python file (beam_model.py) that contains a complex physics function. You want DigiQual to optimise it.\n1. The Existing Physics Solver (beam_model.py) This is your actual engineering code. It doesn’t know anything about CSVs or DigiQual.\n\n# beam_model.py\ndef simulate_beam(length, angle):\n    \"\"\"A complex, proprietary physics calculation.\"\"\"\n    # ... heavy calculations happen here ...\n    signal_strength = (length**3) * 0.5 + angle\n    return signal_strength\n\n2. The Wrapper Script (python_wrapper.py) This script acts as the bridge. It reads the CSV from the command line arguments, loops through the data to run the solver, and saves the result.\n\n# python_wrapper.py\nimport sys\nimport pandas as pd\nfrom beam_model import simulate_beam\n\n\ndef main():\n    # 1. Grab the file paths passed by DigiQual\n    input_csv = sys.argv[1]\n    output_csv = sys.argv[2]\n\n    # 2. Read the design points\n    df = pd.read_csv(input_csv)\n\n    # 3. Run the solver for each row\n    # (We use a lambda function to pass the DataFrame columns into our solver)\n    df[\"Signal\"] = df.apply(\n        lambda row: simulate_beam(row[\"Length\"], row[\"Angle\"]), axis=1\n    )\n\n    # 4. Save the results back for DigiQual to read\n    df.to_csv(output_csv, index=False)\n\n\nif __name__ == \"__main__\":\n    main()\n\n3. The DigiQual Command In your main optimisation script, you define the SOLVER_CMD to call the Python wrapper, passing the placeholders as arguments.\n\n# In your main study script\nSOLVER_CMD = \"python python_wrapper.py {input} {output}\"\n# study.optimise(command=SOLVER_CMD, ...)\n\n\n\n\nExample B: Connecting to MATLAB\nMATLAB can be run in “batch” mode from the command line, making it perfect for automated Active Learning.\n1. The Existing MATLAB Solver (my_matlab_solver.m) Your existing MATLAB function that does the heavy lifting.\n% my_matlab_solver.m\nfunction signal = my_matlab_solver(L, theta)\n    % A complex simulation script\n    signal = (L^3) * 0.5 + theta;\nend\n2. The MATLAB Wrapper Function (matlab_wrapper.m) We write a top-level MATLAB function that reads the CSV, iterates over the design points, and writes the output CSV.\n% matlab_wrapper.m\nfunction matlab_wrapper(input_csv, output_csv)\n    % 1. Read the input coordinates generated by DigiQual\n    df = readtable(input_csv);\n\n    % 2. Initialise an array to hold our results\n    num_rows = height(df);\n    signal_results = zeros(num_rows, 1);\n\n    % 3. Run the solver for each row\n    for i = 1:num_rows\n        L = df.Length(i);\n        theta = df.Angle(i);\n        signal_results(i) = my_matlab_solver(L, theta);\n    end\n\n    % 4. Append the results to the table and save\n    df.Signal = signal_results;\n    writetable(df, output_csv);\n\n    % Exit MATLAB to hand control back to DigiQual\n    exit;\nend\n3. The DigiQual Command We construct a command that launches MATLAB without the graphical interface (-batch), tells it to run our wrapper, and safely injects the {input} and {output} paths as strings.\n\n# In your main study script\nSOLVER_CMD = \"matlab -batch \\\"matlab_wrapper('{input}', '{output}')\\\"\"\n# study.optimise(command=SOLVER_CMD, ...)",
    "crumbs": [
      "Automation & Engines"
    ]
  },
  {
    "objectID": "docs/gui.html",
    "href": "docs/gui.html",
    "title": "Using the GUI",
    "section": "",
    "text": "While the SimulationStudy class offers powerful programmatic control, sometimes you just need a clean visual interface to explore your data without writing scripts. The DigiQual App wraps the core statistical engine into a modern, user-friendly graphical interface, allowing you to run the entire lifecycle—from design to diagnostics to reliability analysis—directly in your browser.",
    "crumbs": [
      "GUI - Shiny App"
    ]
  },
  {
    "objectID": "docs/gui.html#how-to-access-the-app",
    "href": "docs/gui.html#how-to-access-the-app",
    "title": "Using the GUI",
    "section": "1. How to Access the App",
    "text": "1. How to Access the App\nCurrently, the DigiQual app is bundled directly with the Python package. You can launch it locally from your terminal or Jupyter environment using the built-in UI function.\n\n# Launch the local application\nfrom digiqual import dq_ui\n\ndq_ui()\n\nThis will spin up a local server and automatically open the DigiQual dashboard in your default web browser.\nComing Soon: Standalone Executables We are actively developing standalone, double-click executables for both Windows and macOS. In the near future, you or your colleagues will be able to run the complete DigiQual toolkit as a standard desktop application, without needing to install Python, manage virtual environments, or touch a command line.",
    "crumbs": [
      "GUI - Shiny App"
    ]
  },
  {
    "objectID": "docs/gui.html#the-app-workflow",
    "href": "docs/gui.html#the-app-workflow",
    "title": "Using the GUI",
    "section": "2. The App Workflow",
    "text": "2. The App Workflow\nThe app is divided into three main modules that mirror the programmatic workflow you are already familiar with.\n\nModule 1: Experimental Design\nInstead of defining arrays or dictionaries in code, you can use the UI to visually define your experimental parameters.\n\nDefine your input variables (e.g., Length, Angle) and their minimum/maximum ranges.\nSpecify the number of initial samples you want.\nClick generate to instantly build a Latin Hypercube Sample (LHS) framework and download the CSV to feed into your external solver (like MATLAB or Abaqus).\n\n\n\nModule 2: Simulation Diagnostics\nOnce your external physics solver has run, you can upload the resulting CSV back into the app. Use the dropdowns to assign your input columns and your outcome column (e.g., “Signal”). The app will automatically run health checks, such as testing for input coverage gaps.\nAdaptive Refinement: If the app detects a deliberate gap or flawed data, it will flag an issue. Instead of manually guessing where to add points, the UI provides a remediation tool to automatically generate targeted samples to fill the gap. You can download these new points, run them through your solver, and upload the complete dataset.\n\n\nModule 3: Reliability Analysis\nWith a validated dataset, you can construct your Probability of Detection (PoD) curves.\n\nSelect your Parameter of Interest (e.g., “Length”) and set your critical threshold (e.g., 18.0).\nThe app handles the model fitting and bootstrapping in the background.\nIt automatically visualizes the results, displaying both the Signal Response Model (Physics) and the Probability of Detection Curve (Reliability) side-by-side.",
    "crumbs": [
      "GUI - Shiny App"
    ]
  },
  {
    "objectID": "api_reference/pod.predict_local_std.html",
    "href": "api_reference/pod.predict_local_std.html",
    "title": "pod.predict_local_std",
    "section": "",
    "text": "predict_local_std(X, residuals, X_eval, bandwidth)\nEstimates the local standard deviation using Gaussian Kernel Smoothing.\nThis implements a Nadaraya-Watson estimator specifically for the squared residuals to model how noise varies across the input domain (heteroscedasticity).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe source locations (original data inputs).\nrequired\n\n\nresiduals\nnp.ndarray\nThe residuals observed at X.\nrequired\n\n\nX_eval\nnp.ndarray\nThe target locations to estimate variance at.\nrequired\n\n\nbandwidth\nfloat\nThe width of the Gaussian kernel (sigma).\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: An array of standard deviation estimates corresponding to X_eval."
  },
  {
    "objectID": "api_reference/pod.predict_local_std.html#parameters",
    "href": "api_reference/pod.predict_local_std.html#parameters",
    "title": "pod.predict_local_std",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe source locations (original data inputs).\nrequired\n\n\nresiduals\nnp.ndarray\nThe residuals observed at X.\nrequired\n\n\nX_eval\nnp.ndarray\nThe target locations to estimate variance at.\nrequired\n\n\nbandwidth\nfloat\nThe width of the Gaussian kernel (sigma).\nrequired"
  },
  {
    "objectID": "api_reference/pod.predict_local_std.html#returns",
    "href": "api_reference/pod.predict_local_std.html#returns",
    "title": "pod.predict_local_std",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: An array of standard deviation estimates corresponding to X_eval."
  },
  {
    "objectID": "api_reference/pod.infer_best_distribution.html",
    "href": "api_reference/pod.infer_best_distribution.html",
    "title": "pod.infer_best_distribution",
    "section": "",
    "text": "infer_best_distribution(residuals, X, bandwidth)\nSelects the best statistical distribution for the standardized residuals using AIC.\nThis function normalizes residuals by their local standard deviation (Z-scores) and tests them against a suite of candidate distributions (Normal, Gumbel, Logistic, Laplace, t-Student).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nresiduals\nnp.ndarray\nRaw residuals from the mean model.\nrequired\n\n\nX\nnp.ndarray\nInput locations for the residuals.\nrequired\n\n\nbandwidth\nfloat\nBandwidth used for local standardization.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[str, Tuple]\nTuple[str, Tuple]: - best_name: The SciPy name of the best-fitting distribution (e.g., ‘norm’). - best_params: The fitted parameters for that distribution (e.g., loc, scale)."
  },
  {
    "objectID": "api_reference/pod.infer_best_distribution.html#parameters",
    "href": "api_reference/pod.infer_best_distribution.html#parameters",
    "title": "pod.infer_best_distribution",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nresiduals\nnp.ndarray\nRaw residuals from the mean model.\nrequired\n\n\nX\nnp.ndarray\nInput locations for the residuals.\nrequired\n\n\nbandwidth\nfloat\nBandwidth used for local standardization.\nrequired"
  },
  {
    "objectID": "api_reference/pod.infer_best_distribution.html#returns",
    "href": "api_reference/pod.infer_best_distribution.html#returns",
    "title": "pod.infer_best_distribution",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[str, Tuple]\nTuple[str, Tuple]: - best_name: The SciPy name of the best-fitting distribution (e.g., ‘norm’). - best_params: The fitted parameters for that distribution (e.g., loc, scale)."
  },
  {
    "objectID": "api_reference/pod.fit_robust_mean_model.html",
    "href": "api_reference/pod.fit_robust_mean_model.html",
    "title": "pod.fit_robust_mean_model",
    "section": "",
    "text": "fit_robust_mean_model(X, y, max_degree=10, n_folds=10)\nFits regression models (Polynomials and Kriging) and selects the optimal one.\nThis function performs k-fold Cross Validation (CV) to find the model type (Polynomial or Kriging) and parameters (e.g., degree) that minimize the Mean Squared Error (MSE), balancing bias and variance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n1D array of input variable values (e.g., flaw size).\nrequired\n\n\ny\nnp.ndarray\n1D array of outcome values (e.g., signal response).\nrequired\n\n\nmax_degree\nint\nThe maximum polynomial degree to test. Defaults to 10.\n10\n\n\nn_folds\nint\nNumber of folds for Cross Validation. Defaults to 10.\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nAny\nAny\nA fitted scikit-learn model with the following added attributes:\n\n\n\nAny\n- model_type_ (str): Either ‘Polynomial’ or ‘Kriging’.\n\n\n\nAny\n- model_params_ (Any): The selected integer degree or the fitted kernel.\n\n\n\nAny\n- cv_scores_ (dict): The cross-validation MSE scores for all tested models."
  },
  {
    "objectID": "api_reference/pod.fit_robust_mean_model.html#parameters",
    "href": "api_reference/pod.fit_robust_mean_model.html#parameters",
    "title": "pod.fit_robust_mean_model",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n1D array of input variable values (e.g., flaw size).\nrequired\n\n\ny\nnp.ndarray\n1D array of outcome values (e.g., signal response).\nrequired\n\n\nmax_degree\nint\nThe maximum polynomial degree to test. Defaults to 10.\n10\n\n\nn_folds\nint\nNumber of folds for Cross Validation. Defaults to 10.\n10"
  },
  {
    "objectID": "api_reference/pod.fit_robust_mean_model.html#returns",
    "href": "api_reference/pod.fit_robust_mean_model.html#returns",
    "title": "pod.fit_robust_mean_model",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nAny\nAny\nA fitted scikit-learn model with the following added attributes:\n\n\n\nAny\n- model_type_ (str): Either ‘Polynomial’ or ‘Kriging’.\n\n\n\nAny\n- model_params_ (Any): The selected integer degree or the fitted kernel.\n\n\n\nAny\n- cv_scores_ (dict): The cross-validation MSE scores for all tested models."
  },
  {
    "objectID": "api_reference/pod.bootstrap_pod_ci.html",
    "href": "api_reference/pod.bootstrap_pod_ci.html",
    "title": "pod.bootstrap_pod_ci",
    "section": "",
    "text": "bootstrap_pod_ci(\n    X,\n    y,\n    X_eval,\n    threshold,\n    model_type,\n    model_params,\n    bandwidth,\n    dist_info,\n    n_boot=1000,\n)\nEstimates 95% Confidence Bounds for the PoD curve via Bootstrapping.\nThis function resamples the original data with replacement n_boot times. For each resample, it refits the Mean Model (dynamically rebuilding either a Polynomial or Kriging model), recalculates residuals, and generates a new PoD curve. If Kriging is selected, the optimizer is disabled during bootstrapping to remain computationally tractable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nOriginal input data.\nrequired\n\n\ny\nnp.ndarray\nOriginal outcome data.\nrequired\n\n\nX_eval\nnp.ndarray\nGrid points for evaluation.\nrequired\n\n\nthreshold\nfloat\nDetection threshold.\nrequired\n\n\nmodel_type\nstr\nThe type of mean model (‘Polynomial’ or ‘Kriging’).\nrequired\n\n\nmodel_params\nAny\nModel parameters (integer degree for Poly, kernel for Kriging).\nrequired\n\n\nbandwidth\nfloat\nSmoothing bandwidth (fixed from original fit).\nrequired\n\n\ndist_info\nTuple[str, Tuple]\nError distribution (fixed from original fit).\nrequired\n\n\nn_boot\nint\nNumber of bootstrap iterations. Defaults to 1000.\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, np.ndarray]\nTuple[np.ndarray, np.ndarray]: - lower_ci: The 2.5th percentile PoD curve (Lower 95% Bound). - upper_ci: The 97.5th percentile PoD curve (Upper 95% Bound).\n\n\n\n\n\n\n# Generate 95% confidence bounds\nlower, upper = bootstrap_pod_ci(\n    X, y, X_eval, threshold=0.5,\n    model_type='Polynomial', model_params=3,\n    bandwidth=1.5, dist_info=('norm', (0, 1)), n_boot=100\n)"
  },
  {
    "objectID": "api_reference/pod.bootstrap_pod_ci.html#parameters",
    "href": "api_reference/pod.bootstrap_pod_ci.html#parameters",
    "title": "pod.bootstrap_pod_ci",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nOriginal input data.\nrequired\n\n\ny\nnp.ndarray\nOriginal outcome data.\nrequired\n\n\nX_eval\nnp.ndarray\nGrid points for evaluation.\nrequired\n\n\nthreshold\nfloat\nDetection threshold.\nrequired\n\n\nmodel_type\nstr\nThe type of mean model (‘Polynomial’ or ‘Kriging’).\nrequired\n\n\nmodel_params\nAny\nModel parameters (integer degree for Poly, kernel for Kriging).\nrequired\n\n\nbandwidth\nfloat\nSmoothing bandwidth (fixed from original fit).\nrequired\n\n\ndist_info\nTuple[str, Tuple]\nError distribution (fixed from original fit).\nrequired\n\n\nn_boot\nint\nNumber of bootstrap iterations. Defaults to 1000.\n1000"
  },
  {
    "objectID": "api_reference/pod.bootstrap_pod_ci.html#returns",
    "href": "api_reference/pod.bootstrap_pod_ci.html#returns",
    "title": "pod.bootstrap_pod_ci",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, np.ndarray]\nTuple[np.ndarray, np.ndarray]: - lower_ci: The 2.5th percentile PoD curve (Lower 95% Bound). - upper_ci: The 97.5th percentile PoD curve (Upper 95% Bound)."
  },
  {
    "objectID": "api_reference/pod.bootstrap_pod_ci.html#examples",
    "href": "api_reference/pod.bootstrap_pod_ci.html#examples",
    "title": "pod.bootstrap_pod_ci",
    "section": "",
    "text": "# Generate 95% confidence bounds\nlower, upper = bootstrap_pod_ci(\n    X, y, X_eval, threshold=0.5,\n    model_type='Polynomial', model_params=3,\n    bandwidth=1.5, dist_info=('norm', (0, 1)), n_boot=100\n)"
  },
  {
    "objectID": "api_reference/plotting.plot_pod_curve.html",
    "href": "api_reference/plotting.plot_pod_curve.html",
    "title": "plotting.plot_pod_curve",
    "section": "",
    "text": "plot_pod_curve(\n    X_eval,\n    pod_curve,\n    ci_lower=None,\n    ci_upper=None,\n    target_pod=0.9,\n    poi_name='Parameter of Interest',\n    ax=None,\n)\nResult Plot 2: Probability of Detection (The Reliability).\nVisualizes the PoD curve with Bootstrap Confidence Intervals. Equivalent to Figure 11 in the Generalized Method paper.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_eval\nnp.ndarray\nThe grid of points used for the curves.\nrequired\n\n\npod_curve\nnp.ndarray\nThe main PoD estimate (0.0 to 1.0).\nrequired\n\n\nci_lower\nOptional[np.ndarray]\n(Optional) The lower 95% confidence bound.\nNone\n\n\nci_upper\nOptional[np.ndarray]\n(Optional) The upper 95% confidence bound.\nNone\n\n\ntarget_pod\nfloat\nThe target reliability level (usually 0.90) to mark on the plot.\n0.9\n\n\nax\nOptional[plt.Axes]\n(Optional) Matplotlib axes to plot on.\nNone\n\n\n\n\n\n\n# Plot the reliability curve with confidence bounds\nax = plot_pod_curve(\n    X_eval, pod_curve,\n    ci_lower=lower_bound,\n    ci_upper=upper_bound,\n    target_pod=0.90\n)\nplt.show()"
  },
  {
    "objectID": "api_reference/plotting.plot_pod_curve.html#parameters",
    "href": "api_reference/plotting.plot_pod_curve.html#parameters",
    "title": "plotting.plot_pod_curve",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_eval\nnp.ndarray\nThe grid of points used for the curves.\nrequired\n\n\npod_curve\nnp.ndarray\nThe main PoD estimate (0.0 to 1.0).\nrequired\n\n\nci_lower\nOptional[np.ndarray]\n(Optional) The lower 95% confidence bound.\nNone\n\n\nci_upper\nOptional[np.ndarray]\n(Optional) The upper 95% confidence bound.\nNone\n\n\ntarget_pod\nfloat\nThe target reliability level (usually 0.90) to mark on the plot.\n0.9\n\n\nax\nOptional[plt.Axes]\n(Optional) Matplotlib axes to plot on.\nNone"
  },
  {
    "objectID": "api_reference/plotting.plot_pod_curve.html#examples",
    "href": "api_reference/plotting.plot_pod_curve.html#examples",
    "title": "plotting.plot_pod_curve",
    "section": "",
    "text": "# Plot the reliability curve with confidence bounds\nax = plot_pod_curve(\n    X_eval, pod_curve,\n    ci_lower=lower_bound,\n    ci_upper=upper_bound,\n    target_pod=0.90\n)\nplt.show()"
  },
  {
    "objectID": "api_reference/dq_ui.html",
    "href": "api_reference/dq_ui.html",
    "title": "dq_ui",
    "section": "",
    "text": "dq_ui\ndq_ui()\nUser Interface for DigiQual Shiny Application"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.refine.html",
    "href": "api_reference/digiqual.core.SimulationStudy.refine.html",
    "title": "refine",
    "section": "",
    "text": "refine(n_points=10)\nIdentifies gaps or high-variance regions and suggests new simulation points.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_points\nint\nNumber of new samples to suggest per detected issue.\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A DataFrame of recommended new input coordinates.\n\n\n\n\n\n\n# If diagnostics fail, ask for 10 new points to fix it\nnew_samples = study.refine(n_points=10)\nprint(new_samples.head())"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.refine.html#parameters",
    "href": "api_reference/digiqual.core.SimulationStudy.refine.html#parameters",
    "title": "refine",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn_points\nint\nNumber of new samples to suggest per detected issue.\n10"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.refine.html#returns",
    "href": "api_reference/digiqual.core.SimulationStudy.refine.html#returns",
    "title": "refine",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A DataFrame of recommended new input coordinates."
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.refine.html#examples",
    "href": "api_reference/digiqual.core.SimulationStudy.refine.html#examples",
    "title": "refine",
    "section": "",
    "text": "# If diagnostics fail, ask for 10 new points to fix it\nnew_samples = study.refine(n_points=10)\nprint(new_samples.head())"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.optimise.html",
    "href": "api_reference/digiqual.core.SimulationStudy.optimise.html",
    "title": "optimise",
    "section": "",
    "text": "optimise(\n    command,\n    ranges,\n    n_start=20,\n    n_step=10,\n    max_iter=5,\n    input_file='sim_input.csv',\n    output_file='sim_output.csv',\n)\nRuns the automated Active Learning loop (Initialize -&gt; Execute -&gt; Diagnose -&gt; Refine).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncommand\nstr\nSolver command (e.g. “python solver.py {input} {output}”).\nrequired\n\n\nranges\nDict\nInput bounds, e.g. {“Length”: (0, 10)}.\nrequired\n\n\nn_start\nint\nInitial sample size (only if data is empty).\n20\n\n\nn_step\nint\nBatch size for refinement.\n10\n\n\nmax_iter\nint\nMax refinement loops.\n5\n\n\ninput_file\nstr\nTemp input filename.\n'sim_input.csv'\n\n\noutput_file\nstr\nTemp output filename.\n'sim_output.csv'\n\n\n\n\n\n\n# 1. Define the variable ranges\nranges = {\"Length\": (0, 10), \"Angle\": (-45, 45)}\n\nstudy = SimulationStudy(input_cols=[\"Length\", \"Angle\"], outcome_col=\"Signal\")\n\n# 2. Define a \"solver\" command.\n# We use 'python -c' to simulate an external tool (like Ansys/Abaqus)\n# that reads {input}, does math, and saves to {output}.\ncmd = (\n\"python -c \"\n\"'import pandas as pd; \"\n'df=pd.read_csv(\"{input}\"); '\n'df[\"Signal\"] = df[\"Length\"]*2; '\n'df.to_csv(\"{output}\", index=False)'\n\"'\"\n)\n\n# 3. Run the automated loop\nstudy.optimise(\ncommand=cmd,\nranges=ranges,\nmax_iter=3\n)\n\n# 4. View the results\n_ = study.pod()\nstudy.visualise()"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.optimise.html#parameters",
    "href": "api_reference/digiqual.core.SimulationStudy.optimise.html#parameters",
    "title": "optimise",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncommand\nstr\nSolver command (e.g. “python solver.py {input} {output}”).\nrequired\n\n\nranges\nDict\nInput bounds, e.g. {“Length”: (0, 10)}.\nrequired\n\n\nn_start\nint\nInitial sample size (only if data is empty).\n20\n\n\nn_step\nint\nBatch size for refinement.\n10\n\n\nmax_iter\nint\nMax refinement loops.\n5\n\n\ninput_file\nstr\nTemp input filename.\n'sim_input.csv'\n\n\noutput_file\nstr\nTemp output filename.\n'sim_output.csv'"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.optimise.html#examples",
    "href": "api_reference/digiqual.core.SimulationStudy.optimise.html#examples",
    "title": "optimise",
    "section": "",
    "text": "# 1. Define the variable ranges\nranges = {\"Length\": (0, 10), \"Angle\": (-45, 45)}\n\nstudy = SimulationStudy(input_cols=[\"Length\", \"Angle\"], outcome_col=\"Signal\")\n\n# 2. Define a \"solver\" command.\n# We use 'python -c' to simulate an external tool (like Ansys/Abaqus)\n# that reads {input}, does math, and saves to {output}.\ncmd = (\n\"python -c \"\n\"'import pandas as pd; \"\n'df=pd.read_csv(\"{input}\"); '\n'df[\"Signal\"] = df[\"Length\"]*2; '\n'df.to_csv(\"{output}\", index=False)'\n\"'\"\n)\n\n# 3. Run the automated loop\nstudy.optimise(\ncommand=cmd,\nranges=ranges,\nmax_iter=3\n)\n\n# 4. View the results\n_ = study.pod()\nstudy.visualise()"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.add_data.html",
    "href": "api_reference/digiqual.core.SimulationStudy.add_data.html",
    "title": "add_data",
    "section": "",
    "text": "add_data(df)\nIngests raw simulation data, filtering for relevant columns only.\nThis method automatically strips away any columns in df that were not defined in self.inputs or self.outcome during initialization.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame to ingest.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the new data is missing any required input or outcome columns.\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.DataFrame({\n    'Length': [1.0, 2.5, 5.0],\n    'Angle': [0, 15, -10],\n    'Signal': [0.5, 0.8, 1.2]\n})\nstudy.add_data(df)"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.add_data.html#parameters",
    "href": "api_reference/digiqual.core.SimulationStudy.add_data.html#parameters",
    "title": "add_data",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame to ingest.\nrequired"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.add_data.html#raises",
    "href": "api_reference/digiqual.core.SimulationStudy.add_data.html#raises",
    "title": "add_data",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the new data is missing any required input or outcome columns."
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.add_data.html#examples",
    "href": "api_reference/digiqual.core.SimulationStudy.add_data.html#examples",
    "title": "add_data",
    "section": "",
    "text": "import pandas as pd\ndf = pd.DataFrame({\n    'Length': [1.0, 2.5, 5.0],\n    'Angle': [0, 15, -10],\n    'Signal': [0.5, 0.8, 1.2]\n})\nstudy.add_data(df)"
  },
  {
    "objectID": "api_reference/diagnostics.sample_sufficiency.html",
    "href": "api_reference/diagnostics.sample_sufficiency.html",
    "title": "diagnostics.sample_sufficiency",
    "section": "",
    "text": "sample_sufficiency(df, input_cols, outcome_col)\nPerforms statistical tests on sampling sufficiency.\n\n\n\nInput Space Coverage (Gaps)\nModel Fit Stability (CV Score)\nBootstrap Convergence (Coefficient of Variation)\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe simulation data. Will be validated via validate_simulation internally.\nrequired\n\n\ninput_cols\nList[str]\nList of input variable names.\nrequired\n\n\noutcome_col\nstr\nName of the outcome variable.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A table containing pass/fail metrics for each test, including the threshold values evaluated against."
  },
  {
    "objectID": "api_reference/diagnostics.sample_sufficiency.html#runs-3-checks",
    "href": "api_reference/diagnostics.sample_sufficiency.html#runs-3-checks",
    "title": "diagnostics.sample_sufficiency",
    "section": "",
    "text": "Input Space Coverage (Gaps)\nModel Fit Stability (CV Score)\nBootstrap Convergence (Coefficient of Variation)"
  },
  {
    "objectID": "api_reference/diagnostics.sample_sufficiency.html#parameters",
    "href": "api_reference/diagnostics.sample_sufficiency.html#parameters",
    "title": "diagnostics.sample_sufficiency",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe simulation data. Will be validated via validate_simulation internally.\nrequired\n\n\ninput_cols\nList[str]\nList of input variable names.\nrequired\n\n\noutcome_col\nstr\nName of the outcome variable.\nrequired"
  },
  {
    "objectID": "api_reference/diagnostics.sample_sufficiency.html#returns",
    "href": "api_reference/diagnostics.sample_sufficiency.html#returns",
    "title": "diagnostics.sample_sufficiency",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A table containing pass/fail metrics for each test, including the threshold values evaluated against."
  },
  {
    "objectID": "api_reference/core.SimulationStudy.html",
    "href": "api_reference/core.SimulationStudy.html",
    "title": "core.SimulationStudy",
    "section": "",
    "text": "SimulationStudy(input_cols, outcome_col)\nA workflow manager for simulation reliability assessment.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninputs\nList[str]\nList of input variable names.\n\n\noutcome\nstr\nName of the outcome variable.\n\n\ndata\npd.DataFrame\nThe raw simulation data.\n\n\nclean_data\npd.DataFrame\nData that has passed validation.\n\n\nsufficiency_results\npd.DataFrame\nThe latest diagnostic results.\n\n\npod_results\nDict\nResults from the latest PoD analysis.\n\n\nplots\nDict\nStores the latest generated figures.\n\n\n\n\n\n\nfrom digiqual.core import SimulationStudy\nstudy = SimulationStudy(\n    input_cols=['Length', 'Angle'],\n    outcome_col='Signal'\n)\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_data\nIngests raw simulation data, filtering for relevant columns only.\n\n\ndiagnose\nRuns statistical diagnostics to evaluate if the current sample size is sufficient.\n\n\noptimise\nRuns the automated Active Learning loop (Initialize -&gt; Execute -&gt; Diagnose -&gt; Refine).\n\n\npod\nRuns the generalized Probability of Detection (PoD) analysis.\n\n\nrefine\nIdentifies gaps or high-variance regions and suggests new simulation points.\n\n\nvisualise\nGenerates and displays diagnostic plots (Signal Model and PoD Curve)."
  },
  {
    "objectID": "api_reference/core.SimulationStudy.html#attributes",
    "href": "api_reference/core.SimulationStudy.html#attributes",
    "title": "core.SimulationStudy",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ninputs\nList[str]\nList of input variable names.\n\n\noutcome\nstr\nName of the outcome variable.\n\n\ndata\npd.DataFrame\nThe raw simulation data.\n\n\nclean_data\npd.DataFrame\nData that has passed validation.\n\n\nsufficiency_results\npd.DataFrame\nThe latest diagnostic results.\n\n\npod_results\nDict\nResults from the latest PoD analysis.\n\n\nplots\nDict\nStores the latest generated figures."
  },
  {
    "objectID": "api_reference/core.SimulationStudy.html#examples",
    "href": "api_reference/core.SimulationStudy.html#examples",
    "title": "core.SimulationStudy",
    "section": "",
    "text": "from digiqual.core import SimulationStudy\nstudy = SimulationStudy(\n    input_cols=['Length', 'Angle'],\n    outcome_col='Signal'\n)"
  },
  {
    "objectID": "api_reference/core.SimulationStudy.html#methods",
    "href": "api_reference/core.SimulationStudy.html#methods",
    "title": "core.SimulationStudy",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_data\nIngests raw simulation data, filtering for relevant columns only.\n\n\ndiagnose\nRuns statistical diagnostics to evaluate if the current sample size is sufficient.\n\n\noptimise\nRuns the automated Active Learning loop (Initialize -&gt; Execute -&gt; Diagnose -&gt; Refine).\n\n\npod\nRuns the generalized Probability of Detection (PoD) analysis.\n\n\nrefine\nIdentifies gaps or high-variance regions and suggests new simulation points.\n\n\nvisualise\nGenerates and displays diagnostic plots (Signal Model and PoD Curve)."
  },
  {
    "objectID": "api_reference/adaptive.generate_targeted_samples.html",
    "href": "api_reference/adaptive.generate_targeted_samples.html",
    "title": "adaptive.generate_targeted_samples",
    "section": "",
    "text": "generate_targeted_samples(\n    df,\n    input_cols,\n    outcome_col,\n    n_new_per_fix=10,\n    failed_data=None,\n    distance_threshold=0.05,\n)\nActive Learning Engine: Generates new samples based on diagnostic failures.\nIt consumes the results table from sample_sufficiency. - If Input Coverage fails -&gt; Triggers _fill_gaps (Exploration). - If Model Fit or Bootstrap fails -&gt; Triggers _sample_uncertainty (Exploitation).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nCurrent simulation data.\nrequired\n\n\ninput_cols\nList[str]\nInput variable names.\nrequired\n\n\noutcome_col\nstr\nOutcome variable name.\nrequired\n\n\nn_new_per_fix\nint\nNumber of samples to generate per detected issue.\n10\n\n\nfailed_data\nOptional[pd.DataFrame]\nGraveyard of inputs that crashed the solver.\nNone\n\n\ndistance_threshold\nfloat\nMinimum normalized distance to maintain from failed points.\n0.05\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: New recommended samples.\n\n\n\n\n\n\nimport pandas as pd\n# 1. Setup data with a massive gap in 'Length' (0-1, then 9-10)\ndf = pd.DataFrame({'Length': [0.1, 0.9, 9.1, 9.9], 'Signal': [1, 1, 1, 1]})\n\n# 2. Ask for new samples to fix the gap\nnew_pts = generate_targeted_samples(\n    df=df,\n    input_cols=['Length'],\n    outcome_col='Signal',\n    n_new_per_fix=2\n)\nprint(new_pts)\n#    Length Refinement_Reason\n# 0     5.4      Gap in Length\n# 1     3.2      Gap in Length"
  },
  {
    "objectID": "api_reference/adaptive.generate_targeted_samples.html#parameters",
    "href": "api_reference/adaptive.generate_targeted_samples.html#parameters",
    "title": "adaptive.generate_targeted_samples",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nCurrent simulation data.\nrequired\n\n\ninput_cols\nList[str]\nInput variable names.\nrequired\n\n\noutcome_col\nstr\nOutcome variable name.\nrequired\n\n\nn_new_per_fix\nint\nNumber of samples to generate per detected issue.\n10\n\n\nfailed_data\nOptional[pd.DataFrame]\nGraveyard of inputs that crashed the solver.\nNone\n\n\ndistance_threshold\nfloat\nMinimum normalized distance to maintain from failed points.\n0.05"
  },
  {
    "objectID": "api_reference/adaptive.generate_targeted_samples.html#returns",
    "href": "api_reference/adaptive.generate_targeted_samples.html#returns",
    "title": "adaptive.generate_targeted_samples",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: New recommended samples."
  },
  {
    "objectID": "api_reference/adaptive.generate_targeted_samples.html#examples",
    "href": "api_reference/adaptive.generate_targeted_samples.html#examples",
    "title": "adaptive.generate_targeted_samples",
    "section": "",
    "text": "import pandas as pd\n# 1. Setup data with a massive gap in 'Length' (0-1, then 9-10)\ndf = pd.DataFrame({'Length': [0.1, 0.9, 9.1, 9.9], 'Signal': [1, 1, 1, 1]})\n\n# 2. Ask for new samples to fix the gap\nnew_pts = generate_targeted_samples(\n    df=df,\n    input_cols=['Length'],\n    outcome_col='Signal',\n    n_new_per_fix=2\n)\nprint(new_pts)\n#    Length Refinement_Reason\n# 0     5.4      Gap in Length\n# 1     3.2      Gap in Length"
  },
  {
    "objectID": "api_reference/adaptive.run_adaptive_search.html",
    "href": "api_reference/adaptive.run_adaptive_search.html",
    "title": "adaptive.run_adaptive_search",
    "section": "",
    "text": "run_adaptive_search(\n    command,\n    input_cols,\n    outcome_col,\n    ranges,\n    existing_data=None,\n    n_start=20,\n    n_step=10,\n    max_iter=5,\n    input_file='sim_input.csv',\n    output_file='sim_output.csv',\n)\nOrchestrates the Active Learning loop on raw DataFrames.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncommand\nstr\nSolver command template.\nrequired\n\n\ninput_cols\nList[str]\nInput names.\nrequired\n\n\noutcome_col\nstr\nOutcome name.\nrequired\n\n\nranges\nDict\nInput bounds.\nrequired\n\n\nexisting_data\npd.DataFrame\nStart data.\nNone\n\n\nn_start\nint\nInit batch size.\n20\n\n\nmax_iter\nint\nMax loops.\n5\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Final dataset.\n\n\n\n\n\n\n# 1. Define bounds\nranges = {'Length': (0, 10), 'Angle': (-45, 45)}\n\n# 2. Define a command that reads {input} and writes {output}\n# (Here we use python -c to simulate a solver)\ncmd = (\n\"python -c \"\n\"'import pandas as pd; \"\n'df=pd.read_csv(\"{input}\"); '\n'df[\"Signal\"] = df[\"Length\"]*2; '\n'df.to_csv(\"{output}\", index=False)'\n\"'\"\n)\n\n# 3. Run the loop (Init -&gt; Run -&gt; Check -&gt; Refine)\nfinal_df = run_adaptive_search(\n    command=cmd,\n    input_cols=['Length', 'Angle'],\n    outcome_col='Signal',\n    ranges=ranges,\n    max_iter=2,\n    n_start=5\n)\nprint(len(final_df))"
  },
  {
    "objectID": "api_reference/adaptive.run_adaptive_search.html#parameters",
    "href": "api_reference/adaptive.run_adaptive_search.html#parameters",
    "title": "adaptive.run_adaptive_search",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncommand\nstr\nSolver command template.\nrequired\n\n\ninput_cols\nList[str]\nInput names.\nrequired\n\n\noutcome_col\nstr\nOutcome name.\nrequired\n\n\nranges\nDict\nInput bounds.\nrequired\n\n\nexisting_data\npd.DataFrame\nStart data.\nNone\n\n\nn_start\nint\nInit batch size.\n20\n\n\nmax_iter\nint\nMax loops.\n5"
  },
  {
    "objectID": "api_reference/adaptive.run_adaptive_search.html#returns",
    "href": "api_reference/adaptive.run_adaptive_search.html#returns",
    "title": "adaptive.run_adaptive_search",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Final dataset."
  },
  {
    "objectID": "api_reference/adaptive.run_adaptive_search.html#examples",
    "href": "api_reference/adaptive.run_adaptive_search.html#examples",
    "title": "adaptive.run_adaptive_search",
    "section": "",
    "text": "# 1. Define bounds\nranges = {'Length': (0, 10), 'Angle': (-45, 45)}\n\n# 2. Define a command that reads {input} and writes {output}\n# (Here we use python -c to simulate a solver)\ncmd = (\n\"python -c \"\n\"'import pandas as pd; \"\n'df=pd.read_csv(\"{input}\"); '\n'df[\"Signal\"] = df[\"Length\"]*2; '\n'df.to_csv(\"{output}\", index=False)'\n\"'\"\n)\n\n# 3. Run the loop (Init -&gt; Run -&gt; Check -&gt; Refine)\nfinal_df = run_adaptive_search(\n    command=cmd,\n    input_cols=['Length', 'Angle'],\n    outcome_col='Signal',\n    ranges=ranges,\n    max_iter=2,\n    n_start=5\n)\nprint(len(final_df))"
  },
  {
    "objectID": "api_reference/diagnostics.ValidationError.html",
    "href": "api_reference/diagnostics.ValidationError.html",
    "title": "diagnostics.ValidationError",
    "section": "",
    "text": "diagnostics.ValidationError\nValidationError()\nRaised when simulation data fails validation checks."
  },
  {
    "objectID": "api_reference/diagnostics.validate_simulation.html",
    "href": "api_reference/diagnostics.validate_simulation.html",
    "title": "diagnostics.validate_simulation",
    "section": "",
    "text": "validate_simulation(df, input_cols, outcome_col)\nValidates simulation data, coercing to numeric and removing invalid rows.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe raw dataframe containing input columns and the outcome column.\nrequired\n\n\ninput_cols\nList[str]\nList of input variable names.\nrequired\n\n\noutcome_col\nstr\nName of the outcome variable.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, pd.DataFrame]\n* df_clean: The validated, numeric dataframe ready for analysis. * df_removed: A dataframe containing the rows that were dropped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValidationError\nIf columns are missing, types are wrong, or too few valid rows remain.\n\n\n\n\n\n\nimport pandas as pd\n# Create dirty data (includes text and negative values)\ndf = pd.DataFrame({\n    'Length': [1.0, 'BadValue', 5.0],\n    'Signal': [0.5, 0.8, -1.2]\n})\n\n# Validate\nclean, removed = validate_simulation(df, ['Length'], 'Signal')\nprint(f\"Clean rows: {len(clean)}\")\nprint(f\"Removed rows: {len(removed)}\")"
  },
  {
    "objectID": "api_reference/diagnostics.validate_simulation.html#parameters",
    "href": "api_reference/diagnostics.validate_simulation.html#parameters",
    "title": "diagnostics.validate_simulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe raw dataframe containing input columns and the outcome column.\nrequired\n\n\ninput_cols\nList[str]\nList of input variable names.\nrequired\n\n\noutcome_col\nstr\nName of the outcome variable.\nrequired"
  },
  {
    "objectID": "api_reference/diagnostics.validate_simulation.html#returns",
    "href": "api_reference/diagnostics.validate_simulation.html#returns",
    "title": "diagnostics.validate_simulation",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, pd.DataFrame]\n* df_clean: The validated, numeric dataframe ready for analysis. * df_removed: A dataframe containing the rows that were dropped."
  },
  {
    "objectID": "api_reference/diagnostics.validate_simulation.html#raises",
    "href": "api_reference/diagnostics.validate_simulation.html#raises",
    "title": "diagnostics.validate_simulation",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValidationError\nIf columns are missing, types are wrong, or too few valid rows remain."
  },
  {
    "objectID": "api_reference/diagnostics.validate_simulation.html#examples",
    "href": "api_reference/diagnostics.validate_simulation.html#examples",
    "title": "diagnostics.validate_simulation",
    "section": "",
    "text": "import pandas as pd\n# Create dirty data (includes text and negative values)\ndf = pd.DataFrame({\n    'Length': [1.0, 'BadValue', 5.0],\n    'Signal': [0.5, 0.8, -1.2]\n})\n\n# Validate\nclean, removed = validate_simulation(df, ['Length'], 'Signal')\nprint(f\"Clean rows: {len(clean)}\")\nprint(f\"Removed rows: {len(removed)}\")"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.diagnose.html",
    "href": "api_reference/digiqual.core.SimulationStudy.diagnose.html",
    "title": "diagnose",
    "section": "",
    "text": "diagnose()\nRuns statistical diagnostics to evaluate if the current sample size is sufficient.\nChecks for Input Coverage (Gaps), Model Fit Stability, and Bootstrap Convergence.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A summary of diagnostic metrics.\n\n\n\n\n\n\nreport = study.diagnose()\nprint(report[['Test', 'Pass']])"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.diagnose.html#returns",
    "href": "api_reference/digiqual.core.SimulationStudy.diagnose.html#returns",
    "title": "diagnose",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A summary of diagnostic metrics."
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.diagnose.html#examples",
    "href": "api_reference/digiqual.core.SimulationStudy.diagnose.html#examples",
    "title": "diagnose",
    "section": "",
    "text": "report = study.diagnose()\nprint(report[['Test', 'Pass']])"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.pod.html",
    "href": "api_reference/digiqual.core.SimulationStudy.pod.html",
    "title": "pod",
    "section": "",
    "text": "pod(poi_col, threshold, bandwidth_ratio=0.1, n_boot=1000)\nRuns the generalized Probability of Detection (PoD) analysis.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npoi_col\nstr\nThe parameter of interest (e.g., ‘Crack Length’).\nrequired\n\n\nthreshold\nfloat\nThe failure threshold (e.g., 4.0 dB).\nrequired\n\n\nbandwidth_ratio\nfloat\nSmoothing bandwidth fraction (default 0.1).\n0.1\n\n\nn_boot\nint\nBootstrap iterations for confidence bounds.\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nDict\nDict[str, Any]\nDictionary containing models, curves, and fit statistics.\n\n\n\n\n\n\nresults = study.pod(poi_col=\"Length\", threshold=0.5)\nprint(results['dist_info'])"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.pod.html#parameters",
    "href": "api_reference/digiqual.core.SimulationStudy.pod.html#parameters",
    "title": "pod",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npoi_col\nstr\nThe parameter of interest (e.g., ‘Crack Length’).\nrequired\n\n\nthreshold\nfloat\nThe failure threshold (e.g., 4.0 dB).\nrequired\n\n\nbandwidth_ratio\nfloat\nSmoothing bandwidth fraction (default 0.1).\n0.1\n\n\nn_boot\nint\nBootstrap iterations for confidence bounds.\n1000"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.pod.html#returns",
    "href": "api_reference/digiqual.core.SimulationStudy.pod.html#returns",
    "title": "pod",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nDict\nDict[str, Any]\nDictionary containing models, curves, and fit statistics."
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.pod.html#examples",
    "href": "api_reference/digiqual.core.SimulationStudy.pod.html#examples",
    "title": "pod",
    "section": "",
    "text": "results = study.pod(poi_col=\"Length\", threshold=0.5)\nprint(results['dist_info'])"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.visualise.html",
    "href": "api_reference/digiqual.core.SimulationStudy.visualise.html",
    "title": "visualise",
    "section": "",
    "text": "visualise(show=True, save_path=None)\nGenerates and displays diagnostic plots (Signal Model and PoD Curve).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshow\nbool\nIf True, calls plt.show().\nTrue\n\n\nsave_path\nstr\nIf provided, saves figures to disk (e.g., ‘results/run1’). Appends ’_signal.png’ and ’_pod.png’.\nNone\n\n\n\n\n\n\n# Display only\nstudy.visualise()\n\n# Save to disk\nstudy.visualise(show=False, save_path='my_plots')"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.visualise.html#parameters",
    "href": "api_reference/digiqual.core.SimulationStudy.visualise.html#parameters",
    "title": "visualise",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nshow\nbool\nIf True, calls plt.show().\nTrue\n\n\nsave_path\nstr\nIf provided, saves figures to disk (e.g., ‘results/run1’). Appends ’_signal.png’ and ’_pod.png’.\nNone"
  },
  {
    "objectID": "api_reference/digiqual.core.SimulationStudy.visualise.html#examples",
    "href": "api_reference/digiqual.core.SimulationStudy.visualise.html#examples",
    "title": "visualise",
    "section": "",
    "text": "# Display only\nstudy.visualise()\n\n# Save to disk\nstudy.visualise(show=False, save_path='my_plots')"
  },
  {
    "objectID": "api_reference/index.html",
    "href": "api_reference/index.html",
    "title": "All Functions",
    "section": "",
    "text": "The core class for managing reliability studies.\n\n\n\ncore.SimulationStudy\nA workflow manager for simulation reliability assessment.\n\n\n\n\n\n\nLaunch the standalone DigiQual dashboard locally.\n\n\n\ndq_ui\nUser Interface for DigiQual Shiny Application\n\n\n\n\n\n\nStandalone tools for Sampling, Diagnostics, PoD, and Plotting.\n\n\n\nsampling.generate_lhs\nGenerates a Latin Hypercube Sample and scales it to the provided variable bounds.\n\n\ndiagnostics.validate_simulation\nValidates simulation data, coercing to numeric and removing invalid rows.\n\n\ndiagnostics.sample_sufficiency\nPerforms statistical tests on sampling sufficiency.\n\n\ndiagnostics.ValidationError\nRaised when simulation data fails validation checks.\n\n\nadaptive.run_adaptive_search\nOrchestrates the Active Learning loop on raw DataFrames.\n\n\nadaptive.generate_targeted_samples\nActive Learning Engine: Generates new samples based on diagnostic failures.\n\n\npod.fit_robust_mean_model\nFits regression models (Polynomials and Kriging) and selects the optimal one.\n\n\npod.fit_variance_model\nCalculates residuals and prepares the grid for variance estimation.\n\n\npod.infer_best_distribution\nSelects the best statistical distribution for the standardized residuals using AIC.\n\n\npod.plot_model_selection\nGenerates a normalized bar chart of the Bias-Variance Tradeoff from CV scores,\n\n\npod.predict_local_std\nEstimates the local standard deviation using Gaussian Kernel Smoothing.\n\n\npod.compute_pod_curve\nCalculates the Probability of Detection (PoD) curve.\n\n\npod.bootstrap_pod_ci\nEstimates 95% Confidence Bounds for the PoD curve via Bootstrapping.\n\n\nplotting.plot_signal_model\nDiagnostic Plot 1: Signal vs Parameter of Interest (The Physics).\n\n\nplotting.plot_pod_curve\nResult Plot 2: Probability of Detection (The Reliability).",
    "crumbs": [
      "All Functions"
    ]
  },
  {
    "objectID": "api_reference/index.html#standard-workflow",
    "href": "api_reference/index.html#standard-workflow",
    "title": "All Functions",
    "section": "",
    "text": "The core class for managing reliability studies.\n\n\n\ncore.SimulationStudy\nA workflow manager for simulation reliability assessment.",
    "crumbs": [
      "All Functions"
    ]
  },
  {
    "objectID": "api_reference/index.html#graphical-user-interface",
    "href": "api_reference/index.html#graphical-user-interface",
    "title": "All Functions",
    "section": "",
    "text": "Launch the standalone DigiQual dashboard locally.\n\n\n\ndq_ui\nUser Interface for DigiQual Shiny Application",
    "crumbs": [
      "All Functions"
    ]
  },
  {
    "objectID": "api_reference/index.html#functional-api",
    "href": "api_reference/index.html#functional-api",
    "title": "All Functions",
    "section": "",
    "text": "Standalone tools for Sampling, Diagnostics, PoD, and Plotting.\n\n\n\nsampling.generate_lhs\nGenerates a Latin Hypercube Sample and scales it to the provided variable bounds.\n\n\ndiagnostics.validate_simulation\nValidates simulation data, coercing to numeric and removing invalid rows.\n\n\ndiagnostics.sample_sufficiency\nPerforms statistical tests on sampling sufficiency.\n\n\ndiagnostics.ValidationError\nRaised when simulation data fails validation checks.\n\n\nadaptive.run_adaptive_search\nOrchestrates the Active Learning loop on raw DataFrames.\n\n\nadaptive.generate_targeted_samples\nActive Learning Engine: Generates new samples based on diagnostic failures.\n\n\npod.fit_robust_mean_model\nFits regression models (Polynomials and Kriging) and selects the optimal one.\n\n\npod.fit_variance_model\nCalculates residuals and prepares the grid for variance estimation.\n\n\npod.infer_best_distribution\nSelects the best statistical distribution for the standardized residuals using AIC.\n\n\npod.plot_model_selection\nGenerates a normalized bar chart of the Bias-Variance Tradeoff from CV scores,\n\n\npod.predict_local_std\nEstimates the local standard deviation using Gaussian Kernel Smoothing.\n\n\npod.compute_pod_curve\nCalculates the Probability of Detection (PoD) curve.\n\n\npod.bootstrap_pod_ci\nEstimates 95% Confidence Bounds for the PoD curve via Bootstrapping.\n\n\nplotting.plot_signal_model\nDiagnostic Plot 1: Signal vs Parameter of Interest (The Physics).\n\n\nplotting.plot_pod_curve\nResult Plot 2: Probability of Detection (The Reliability).",
    "crumbs": [
      "All Functions"
    ]
  },
  {
    "objectID": "api_reference/plotting.plot_signal_model.html",
    "href": "api_reference/plotting.plot_signal_model.html",
    "title": "plotting.plot_signal_model",
    "section": "",
    "text": "plot_signal_model(\n    X,\n    y,\n    X_eval,\n    mean_curve,\n    threshold,\n    local_std=None,\n    poi_name='Parameter of Interest',\n    ax=None,\n)\nDiagnostic Plot 1: Signal vs Parameter of Interest (The Physics).\nVisualizes the raw simulation data, the fitted mean model, and the detection threshold. Equivalent to Figure 6/12 in the Generalized Method paper.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe raw PoI.\nrequired\n\n\ny\nnp.ndarray\nThe raw signal responses.\nrequired\n\n\nX_eval\nnp.ndarray\nThe grid of points used for the curves.\nrequired\n\n\nmean_curve\nnp.ndarray\nThe predicted mean response at X_eval.\nrequired\n\n\nthreshold\nfloat\nThe detection threshold (horizontal line).\nrequired\n\n\nlocal_std\nOptional[np.ndarray]\n(Optional) The predicted standard deviation at X_eval. If provided, adds 95% prediction bounds to show noise structure.\nNone\n\n\nax\nOptional[plt.Axes]\n(Optional) Matplotlib axes to plot on. Creates new if None.\nNone\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Plot the physics model\nax = plot_signal_model(\n    X, y, X_eval, mean_curve,\n    threshold=3.0,\n    local_std=std_curve,\n    poi_name=\"Crack Length (mm)\"\n)\nplt.show()"
  },
  {
    "objectID": "api_reference/plotting.plot_signal_model.html#parameters",
    "href": "api_reference/plotting.plot_signal_model.html#parameters",
    "title": "plotting.plot_signal_model",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe raw PoI.\nrequired\n\n\ny\nnp.ndarray\nThe raw signal responses.\nrequired\n\n\nX_eval\nnp.ndarray\nThe grid of points used for the curves.\nrequired\n\n\nmean_curve\nnp.ndarray\nThe predicted mean response at X_eval.\nrequired\n\n\nthreshold\nfloat\nThe detection threshold (horizontal line).\nrequired\n\n\nlocal_std\nOptional[np.ndarray]\n(Optional) The predicted standard deviation at X_eval. If provided, adds 95% prediction bounds to show noise structure.\nNone\n\n\nax\nOptional[plt.Axes]\n(Optional) Matplotlib axes to plot on. Creates new if None.\nNone"
  },
  {
    "objectID": "api_reference/plotting.plot_signal_model.html#examples",
    "href": "api_reference/plotting.plot_signal_model.html#examples",
    "title": "plotting.plot_signal_model",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\n# Plot the physics model\nax = plot_signal_model(\n    X, y, X_eval, mean_curve,\n    threshold=3.0,\n    local_std=std_curve,\n    poi_name=\"Crack Length (mm)\"\n)\nplt.show()"
  },
  {
    "objectID": "api_reference/pod.compute_pod_curve.html",
    "href": "api_reference/pod.compute_pod_curve.html",
    "title": "pod.compute_pod_curve",
    "section": "",
    "text": "compute_pod_curve(\n    X_eval,\n    mean_model,\n    X,\n    residuals,\n    bandwidth,\n    dist_info,\n    threshold,\n)\nCalculates the Probability of Detection (PoD) curve.\nCombines the Mean Model, Variance Model, and Error Distribution to compute the probability that the signal exceeds the threshold at every point in X_eval.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_eval\nnp.ndarray\nThe grid points to calculate PoD for.\nrequired\n\n\nmean_model\nAny\nThe fitted sklearn mean response model.\nrequired\n\n\nX\nnp.ndarray\nOriginal input data (needed for variance prediction).\nrequired\n\n\nresiduals\nnp.ndarray\nOriginal residuals (needed for variance prediction).\nrequired\n\n\nbandwidth\nfloat\nSmoothing bandwidth.\nrequired\n\n\ndist_info\nTuple[str, Tuple]\nThe (name, params) of the error distribution.\nrequired\n\n\nthreshold\nfloat\nThe detection threshold value.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, np.ndarray]\nTuple[np.ndarray, np.ndarray]: - pod_curve: Array of probabilities [0, 1] for each point in X_eval. - mean_curve: Array of mean signal response values for X_eval.\n\n\n\n\n\n\n# Assuming we have fitted models (mean_model) and data (X, residuals)\n# Calculate the PoD curve for a threshold of 0.5\npod, mean_resp = compute_pod_curve(\n    X_eval=np.linspace(0, 10, 100),\n    mean_model=mean_model,\n    X=X,\n    residuals=residuals,\n    bandwidth=1.5,\n    dist_info=('norm', (0, 1)),\n    threshold=0.5\n)"
  },
  {
    "objectID": "api_reference/pod.compute_pod_curve.html#parameters",
    "href": "api_reference/pod.compute_pod_curve.html#parameters",
    "title": "pod.compute_pod_curve",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_eval\nnp.ndarray\nThe grid points to calculate PoD for.\nrequired\n\n\nmean_model\nAny\nThe fitted sklearn mean response model.\nrequired\n\n\nX\nnp.ndarray\nOriginal input data (needed for variance prediction).\nrequired\n\n\nresiduals\nnp.ndarray\nOriginal residuals (needed for variance prediction).\nrequired\n\n\nbandwidth\nfloat\nSmoothing bandwidth.\nrequired\n\n\ndist_info\nTuple[str, Tuple]\nThe (name, params) of the error distribution.\nrequired\n\n\nthreshold\nfloat\nThe detection threshold value.\nrequired"
  },
  {
    "objectID": "api_reference/pod.compute_pod_curve.html#returns",
    "href": "api_reference/pod.compute_pod_curve.html#returns",
    "title": "pod.compute_pod_curve",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, np.ndarray]\nTuple[np.ndarray, np.ndarray]: - pod_curve: Array of probabilities [0, 1] for each point in X_eval. - mean_curve: Array of mean signal response values for X_eval."
  },
  {
    "objectID": "api_reference/pod.compute_pod_curve.html#examples",
    "href": "api_reference/pod.compute_pod_curve.html#examples",
    "title": "pod.compute_pod_curve",
    "section": "",
    "text": "# Assuming we have fitted models (mean_model) and data (X, residuals)\n# Calculate the PoD curve for a threshold of 0.5\npod, mean_resp = compute_pod_curve(\n    X_eval=np.linspace(0, 10, 100),\n    mean_model=mean_model,\n    X=X,\n    residuals=residuals,\n    bandwidth=1.5,\n    dist_info=('norm', (0, 1)),\n    threshold=0.5\n)"
  },
  {
    "objectID": "api_reference/pod.fit_variance_model.html",
    "href": "api_reference/pod.fit_variance_model.html",
    "title": "pod.fit_variance_model",
    "section": "",
    "text": "fit_variance_model(X, y, mean_model, bandwidth_ratio=0.1, n_eval_points=100)\nCalculates residuals and prepares the grid for variance estimation.\nThis acts as the setup phase for the heteroscedasticity model. It computes the raw residuals from the mean model and defines the smoothing bandwidth.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe original input data.\nrequired\n\n\ny\nnp.ndarray\nThe original outcome data.\nrequired\n\n\nmean_model\nAny\nThe fitted sklearn pipeline from fit_robust_mean_model.\nrequired\n\n\nbandwidth_ratio\nfloat\nThe kernel smoothing window size as a fraction of the data range (X_max - X_min). Defaults to 0.1.\n0.1\n\n\nn_eval_points\nint\nNumber of points in the evaluation grid. Defaults to 100.\n100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, float, np.ndarray]\nTuple[np.ndarray, float, np.ndarray]: - residuals: Raw differences between y and the mean model prediction. - bandwidth: The calculated smoothing window size (absolute units). - X_eval: A linearly spaced grid over the X domain for plotting/evaluation."
  },
  {
    "objectID": "api_reference/pod.fit_variance_model.html#parameters",
    "href": "api_reference/pod.fit_variance_model.html#parameters",
    "title": "pod.fit_variance_model",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe original input data.\nrequired\n\n\ny\nnp.ndarray\nThe original outcome data.\nrequired\n\n\nmean_model\nAny\nThe fitted sklearn pipeline from fit_robust_mean_model.\nrequired\n\n\nbandwidth_ratio\nfloat\nThe kernel smoothing window size as a fraction of the data range (X_max - X_min). Defaults to 0.1.\n0.1\n\n\nn_eval_points\nint\nNumber of points in the evaluation grid. Defaults to 100.\n100"
  },
  {
    "objectID": "api_reference/pod.fit_variance_model.html#returns",
    "href": "api_reference/pod.fit_variance_model.html#returns",
    "title": "pod.fit_variance_model",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, float, np.ndarray]\nTuple[np.ndarray, float, np.ndarray]: - residuals: Raw differences between y and the mean model prediction. - bandwidth: The calculated smoothing window size (absolute units). - X_eval: A linearly spaced grid over the X domain for plotting/evaluation."
  },
  {
    "objectID": "api_reference/pod.plot_model_selection.html",
    "href": "api_reference/pod.plot_model_selection.html",
    "title": "pod.plot_model_selection",
    "section": "",
    "text": "pod.plot_model_selection\nplot_model_selection(cv_scores)\nGenerates a normalized bar chart of the Bias-Variance Tradeoff from CV scores, alongside a sorted table of the exact MSE values in best-fit order."
  },
  {
    "objectID": "api_reference/sampling.generate_lhs.html",
    "href": "api_reference/sampling.generate_lhs.html",
    "title": "sampling.generate_lhs",
    "section": "",
    "text": "generate_lhs(n, ranges, seed=None)\nGenerates a Latin Hypercube Sample and scales it to the provided variable bounds.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe total number of samples to generate.\nrequired\n\n\nranges\nUnion[pd.DataFrame, Dict]\nDefinition of input variables. - Dict Format: {‘Name’: (Min, Max), …} e.g. {‘Length’: (0, 10)} - DataFrame Format: Columns [‘Name’, ‘Min’, ‘Max’]\nrequired\n\n\nseed\nint\nSets the random seed for reproducibility. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A dataframe containing the scaled simulation parameters, where column names correspond to the keys in ranges. Returns an empty DataFrame if ranges is empty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf required columns are missing, types are incorrect, or Min &gt;= Max.\n\n\n\nTypeError\nIf ranges is not a Dictionary or DataFrame.\n\n\n\n\n\n\nUsing a dictionary (Recommended):\nranges = {'Length': (0, 10), 'Angle': (0, 90)}\ndf = generate_lhs(n=3, ranges=ranges, seed=42)\nprint(df.round(2))\n#    Length  Angle\n# 0    3.75  85.54\n# 1    9.51  13.56\n# 2    7.32  54.17\nUsing a DataFrame (Legacy/Advanced):\nimport pandas as pd\nvars_df = pd.DataFrame([\n    {'Name': 'Length', 'Min': 0, 'Max': 10},\n    {'Name': 'Angle', 'Min': 0, 'Max': 90}\n])\ndf = generate_lhs(n=3, ranges=vars_df, seed=42)"
  },
  {
    "objectID": "api_reference/sampling.generate_lhs.html#parameters",
    "href": "api_reference/sampling.generate_lhs.html#parameters",
    "title": "sampling.generate_lhs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe total number of samples to generate.\nrequired\n\n\nranges\nUnion[pd.DataFrame, Dict]\nDefinition of input variables. - Dict Format: {‘Name’: (Min, Max), …} e.g. {‘Length’: (0, 10)} - DataFrame Format: Columns [‘Name’, ‘Min’, ‘Max’]\nrequired\n\n\nseed\nint\nSets the random seed for reproducibility. Defaults to None.\nNone"
  },
  {
    "objectID": "api_reference/sampling.generate_lhs.html#returns",
    "href": "api_reference/sampling.generate_lhs.html#returns",
    "title": "sampling.generate_lhs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A dataframe containing the scaled simulation parameters, where column names correspond to the keys in ranges. Returns an empty DataFrame if ranges is empty."
  },
  {
    "objectID": "api_reference/sampling.generate_lhs.html#raises",
    "href": "api_reference/sampling.generate_lhs.html#raises",
    "title": "sampling.generate_lhs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf required columns are missing, types are incorrect, or Min &gt;= Max.\n\n\n\nTypeError\nIf ranges is not a Dictionary or DataFrame."
  },
  {
    "objectID": "api_reference/sampling.generate_lhs.html#examples",
    "href": "api_reference/sampling.generate_lhs.html#examples",
    "title": "sampling.generate_lhs",
    "section": "",
    "text": "Using a dictionary (Recommended):\nranges = {'Length': (0, 10), 'Angle': (0, 90)}\ndf = generate_lhs(n=3, ranges=ranges, seed=42)\nprint(df.round(2))\n#    Length  Angle\n# 0    3.75  85.54\n# 1    9.51  13.56\n# 2    7.32  54.17\nUsing a DataFrame (Legacy/Advanced):\nimport pandas as pd\nvars_df = pd.DataFrame([\n    {'Name': 'Length', 'Min': 0, 'Max': 10},\n    {'Name': 'Angle', 'Min': 0, 'Max': 90}\n])\ndf = generate_lhs(n=3, ranges=vars_df, seed=42)"
  },
  {
    "objectID": "docs/install.html",
    "href": "docs/install.html",
    "title": "Installation",
    "section": "",
    "text": "You can install digiqual directly from PyPI (or GitHub).\n\n\nIf you are managing a project with uv, add digiqual as a dependency:\n\nTo install the latest stable release (v0.12.0):\n\nuv add \"digiqual\"\n\nTo install the latest development version (main branch from github):\n\nuv add \"digiqual @ git+https://github.com/JGIBristol/digiqual.git\"\nIf you just want to install it into a virtual environment without modifying a project file (e.g., for a quick script), use pip interface:\nuv pip install digiqual\"\n\n\n\nTo install the latest stable release (v0.12.0):\npip install digiqual\nTo install the latest development version:\npip install \"git+https://github.com/JGIBristol/digiqual.git\"",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "docs/install.html#option-1-install-via-uv-recommended",
    "href": "docs/install.html#option-1-install-via-uv-recommended",
    "title": "Installation",
    "section": "",
    "text": "If you are managing a project with uv, add digiqual as a dependency:\n\nTo install the latest stable release (v0.12.0):\n\nuv add \"digiqual\"\n\nTo install the latest development version (main branch from github):\n\nuv add \"digiqual @ git+https://github.com/JGIBristol/digiqual.git\"\nIf you just want to install it into a virtual environment without modifying a project file (e.g., for a quick script), use pip interface:\nuv pip install digiqual\"",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "docs/install.html#option-2-install-via-standard-pip",
    "href": "docs/install.html#option-2-install-via-standard-pip",
    "title": "Installation",
    "section": "",
    "text": "To install the latest stable release (v0.12.0):\npip install digiqual\nTo install the latest development version:\npip install \"git+https://github.com/JGIBristol/digiqual.git\"",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "docs/qs_class.html",
    "href": "docs/qs_class.html",
    "title": "SimulationStudy Class",
    "section": "",
    "text": "Use the SimulationStudy manager to handle the entire lifecycle: storage, diagnostics, and active refinement. This allows you to automatically fix issues in your design.\nIn this example, we will intentionally feed the study “bad” data (with a large gap in the input space) to demonstrate how the active learning module identifies and fixes the problem.\n\n1. Setup & “Flawed” Data Generation\nFirst, we define our physics simulation and create a dataset with a deliberate gap (missing data between 3mm and 7mm).\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Markdown\n\nfrom digiqual import SimulationStudy\n\n\n# --- Define the Physics ---\ndef apply_physics(df):\n    \"\"\"Simulates a signal with quadratic trends and heteroscedastic noise.\"\"\"\n    # 1. Base Signal: Quadratic trend (2*L + 0.5*L^2)\n    # 2. Angle Penalty: Misalignment (-0.1*Angle) reduces signal\n    signal = (\n        10.0\n        + (2.0 * df[\"Length\"])\n        + (0.5 * df[\"Length\"] ** 2)\n        - (0.1 * np.abs(df[\"Angle\"]))\n    )\n\n    # 3. Heteroscedastic Noise: Higher roughness = More scatter\n    noise_scale = 0.5 + (1.5 * df[\"Roughness\"])\n    noise = np.random.normal(loc=0, scale=noise_scale, size=len(df))\n\n    return signal + noise\n\n\n# --- Create Flawed Data (Gap between 3mm and 7mm) ---\ndf1 = pd.DataFrame(\n    {\n        \"Length\": np.random.uniform(0.1, 3.0, 40),\n        \"Angle\": np.random.uniform(-10, 10, 40),\n        \"Roughness\": np.random.uniform(0, 0.5, 40),\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"Length\": np.random.uniform(7.0, 10.0, 40),\n        \"Angle\": np.random.uniform(-10, 10, 40),\n        \"Roughness\": np.random.uniform(0, 0.5, 40),\n    }\n)\n\n# Combine and Initialize Study\ndf_initial = pd.concat([df1, df2], ignore_index=True)\ndf_initial[\"Signal\"] = apply_physics(df_initial)\n\nstudy = SimulationStudy(\n    input_cols=[\"Length\", \"Angle\", \"Roughness\"], outcome_col=\"Signal\"\n)\nstudy.add_data(df_initial)\n\nData updated. Total rows: 80\n\n\n\n\n2. Diagnosis (Detecting the Issue)\nWe now ask the study manager to diagnose the health of our experiment. We expect it to flag the Input Coverage test because of the gap we created.\n\nstudy.diagnose()\n\nRunning validation...\nValidation passed. 80 valid rows ready.\nChecking sample sufficiency...\n\n\n\n\n\n\n\n\n\nTest\nVariable\nMetric\nValue\nThreshold\nPass\n\n\n\n\n0\nInput Coverage\nLength\nMax Gap Ratio\n0.4140\n&lt; 0.20\nFalse\n\n\n1\nInput Coverage\nAngle\nMax Gap Ratio\n0.0636\n&lt; 0.20\nTrue\n\n\n2\nInput Coverage\nRoughness\nMax Gap Ratio\n0.0541\n&lt; 0.20\nTrue\n\n\n3\nModel Fit (CV)\nSignal\nMean R2 Score\n0.9977\n&gt; 0.50\nTrue\n\n\n4\nBootstrap Convergence\nSignal\nAvg CV (Rel Std Dev)\n0.0132\n&lt; 0.15\nTrue\n\n\n5\nBootstrap Convergence\nSignal\nMax CV (Rel Std Dev)\n0.0257\n&lt; 0.30\nTrue\n\n\n\n\n\n\n\n\n\n3. Adaptive Refinement (The Fix)\nInstead of manually guessing where to add points, we use refine(). The study manager automatically detects the gap and generates targeted samples to fill it. In this example, we run the apply_physics() function, but in reality, users should feed the new samples dataframe into their simulation engine to calculate the outcome variable. Users can then add those results into the SimulationStudy class using the add_data() method.\n\n# Generate active learning samples (20 points to fill the gap)\nnew_samples = study.refine(n_points=20)\n\nif not new_samples.empty:\n    # Apply the exact same physics model to the new points\n    new_samples['Signal'] = apply_physics(new_samples)\n\n    # Add back to study\n    study.add_data(new_samples)\n\nDiagnostics flagged issues. Initiating Active Learning...\n -&gt; Strategy: Exploration (Filling gaps in Length)\nData updated. Total rows: 100\n\n\nNow we verify that the issue is resolved:\n\n# Re-run diagnostics to confirm the green light\nstudy.diagnose()\n\nRunning validation...\nValidation passed. 100 valid rows ready.\nChecking sample sufficiency...\n\n\n\n\n\n\n\n\n\nTest\nVariable\nMetric\nValue\nThreshold\nPass\n\n\n\n\n0\nInput Coverage\nLength\nMax Gap Ratio\n0.0533\n&lt; 0.20\nTrue\n\n\n1\nInput Coverage\nAngle\nMax Gap Ratio\n0.0636\n&lt; 0.20\nTrue\n\n\n2\nInput Coverage\nRoughness\nMax Gap Ratio\n0.0504\n&lt; 0.20\nTrue\n\n\n3\nModel Fit (CV)\nSignal\nMean R2 Score\n0.9970\n&gt; 0.50\nTrue\n\n\n4\nBootstrap Convergence\nSignal\nAvg CV (Rel Std Dev)\n0.0142\n&lt; 0.15\nTrue\n\n\n5\nBootstrap Convergence\nSignal\nMax CV (Rel Std Dev)\n0.0280\n&lt; 0.30\nTrue\n\n\n\n\n\n\n\n\n\n4. Reliability Analysis\nWith a validated dataset, we can now run the complete PoD pipeline. The pod() method handles model fitting, distribution selection, and bootstrapping in a single call.\nBecause the SimulationStudy object manages the state, we can run the analysis and immediately generate standard diagnostics without needing to manually handle the output data.\n\n# 1. Run Analysis (Threshold = 18 dB)\n_ = study.pod(poi_col=\"Length\", threshold=18.0)\n\n--- Starting Reliability Analysis (PoI: Length) ---\n1. Selecting Mean Model (Cross-Validation)...\n-&gt; Selected Model: Polynomial (Degree 2)\n2. Fitting Variance Model (Kernel Smoothing)...\n   -&gt; Smoothing Bandwidth: 0.9756\n3. Inferring Error Distribution (AIC)...\n   -&gt; Selected Distribution: norm\n4. Computing PoD Curve...\n5. Running Bootstrap (1000 iterations)...\n   -&gt; a90/95 Reliability Index: 2.877\n--- Analysis Complete ---\n\n\n# 2. Visualize Results\nstudy.visualise()\n\n\n\n\n\n\n\n\n\n\n\n(a) Best Fitting Model (Statistics)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Signal Response Model (Physics)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Probability of Detection Curve (Reliability)\n\n\n\n\n\n\n\nFigure 1: Reliability Analysis Results\n\n\n\n\n\nAppendix: Full Script\n\nimport numpy as np\nimport pandas as pd\n\nfrom digiqual import SimulationStudy\nfrom digiqual.sampling import generate_lhs\n\n\n# --- Define the Physics ---\ndef apply_physics(df):\n    \"\"\"Simulates a signal with quadratic trends and heteroscedastic noise.\"\"\"\n    # 1. Base Signal: Quadratic trend (2*L + 0.5*L^2)\n    # 2. Angle Penalty: Misalignment (-0.1*Angle) reduces signal\n    signal = (\n        10.0\n        + (2.0 * df[\"Length\"])\n        + (0.5 * df[\"Length\"] ** 2)\n        - (0.1 * np.abs(df[\"Angle\"]))\n    )\n\n    # 3. Heteroscedastic Noise: Higher roughness = More scatter\n    noise_scale = 0.5 + (1.5 * df[\"Roughness\"])\n    noise = np.random.normal(loc=0, scale=noise_scale, size=len(df))\n\n    return signal + noise\n\n\n# --- Create and analyse a complete dataset ---\nvars_df = pd.DataFrame(\n    [\n        {\"Name\": \"Length\", \"Min\": 0.1, \"Max\": 10},\n        {\"Name\": \"Angle\", \"Min\": -90, \"Max\": 90},\n        {\"Name\": \"Roughness\", \"Min\": 0, \"Max\": 1},\n    ]\n)\n\ndf = generate_lhs(ranges=vars_df, n=50)\n\ndf[\"Signal\"] = apply_physics(df)\n\nstudy = SimulationStudy(\n    input_cols=[\"Length\", \"Angle\", \"Roughness\"], outcome_col=\"Signal\"\n)\nstudy.add_data(df)\n\nstudy.diagnose()\n\nstudy.refine()\n\n_ = study.pod(poi_col=\"Length\", threshold=15)\n\nstudy.visualise()",
    "crumbs": [
      "Quick Start"
    ]
  }
]